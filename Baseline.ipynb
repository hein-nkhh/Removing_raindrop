{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Raindrop Removal System with Day/Night Handling\n",
    "This notebook implements and improves upon a paper for unsupervised raindrop removal.\n",
    "\n",
    "## Library Explanations\n",
    "- `torch`: PyTorch is an open-source machine learning library used for applications such as computer vision and natural language processing.\n",
    "- `torch.nn`: A subpackage of PyTorch that provides modules and classes to build neural networks.\n",
    "- `torch.optim`: A subpackage of PyTorch that contains optimization algorithms.\n",
    "- `torch.utils.data`: A PyTorch package that provides utilities for data loading and processing.\n",
    "- `torchvision`: A PyTorch package that provides tools for image processing, including pre-trained models and transformations.\n",
    "- `lpips`: A library for computing the Learned Perceptual Image Patch Similarity metric, used for evaluating image quality.\n",
    "- `numpy`: A library for numerical computations in Python.\n",
    "- `skimage.metrics`: A module from the scikit-image library that provides functions to measure image quality, such as PSNR and SSIM.\n",
    "- `os`: A standard library in Python for interacting with the operating system.\n",
    "- `PIL`: The Python Imaging Library, used for opening, manipulating, and saving images.\n",
    "- `tqdm`: A library for creating progress bars in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lpips\n",
    "#!pip install lpips (Kaggle)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unsupervised Raindrop Removal System with Day/Night Handling\n",
    "Paper Implementation + Improvements\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms, models\n",
    "from torchvision.utils import save_image\n",
    "import lpips\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------- CONFIG ---------------------\n",
    "The following configuration parameters are used in the implementation:\n",
    "- `device`: Specifies whether to use a GPU (if available) or CPU for training.\n",
    "- `batch_size`: The number of samples processed before the model is updated.\n",
    "- `num_epochs`: The number of complete passes through the training dataset.\n",
    "- `lr`: The learning rate for the optimizer.\n",
    "- `lambda_cycle`: The weight for the cycle consistency loss.\n",
    "- `lambda_percep`: The weight for the perceptual loss.\n",
    "- `lambda_blur`: The weight for the blur loss.\n",
    "- `val_ratio`: The ratio of the dataset to be used for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 6\n",
    "num_epochs = 50\n",
    "lr = 0.0001\n",
    "lambda_cycle = 10\n",
    "lambda_percep = 5\n",
    "lambda_blur = 2\n",
    "val_ratio = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------- DATASET ---------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDomainRaindropDataset(Dataset):\n",
    "    def __init__(self, root_dir, mode = 'train', test_split_ratio = 0.1):\n",
    "        self.mode = mode\n",
    "        self.clear_path = os.path.join(root_dir, 'Clear')\n",
    "        self.blur_path = os.path.join(root_dir, 'Blur')\n",
    "        self.drop_path = os.path.join(root_dir, 'Drop')\n",
    "        \n",
    "        self.clear_images = sorted(glob.glob(os.path.join(self.clear_path, '**', '*.*'), recursive=True))\n",
    "        self.blur_images = sorted(glob.glob(os.path.join(self.blur_path, '**', '*.*'), recursive=True))\n",
    "        self.drop_images = sorted(glob.glob(os.path.join(self.drop_path, '**', '*.*'), recursive=True))\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            #transforms.RandomHorizontalFlip() if mode == 'train' else transforms.Lambda(lambda x: x),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        super().__init__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(len(self.clear_images), len(self.blur_images), len(self.drop_images))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            clear = self.transform(Image.open(self.clear_images[idx % len(self.clear_images)]).convert('RGB'))\n",
    "            blur = self.transform(Image.open(self.blur_images[idx % len(self.blur_images)]).convert('RGB'))\n",
    "            drop = self.transform(Image.open(self.drop_images[idx % len(self.drop_images)]).convert('RGB'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image: {e}\")\n",
    "            return self[(idx + 1) % len(self)]\n",
    "            \n",
    "        return {'clear': clear, 'blur': blur, 'drop': drop}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------- MODELS ---------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainAwareGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Shared encoder\n",
    "        self.enc_conv1 = nn.Conv2d(3, 64, 4, 2, 1)\n",
    "        self.enc_conv2 = nn.Conv2d(64, 128, 4, 2, 1)\n",
    "        self.enc_conv3 = nn.Conv2d(128, 256, 4, 2, 1)\n",
    "        self.enc_conv4 = nn.Conv2d(256, 512, 4, 2, 1)\n",
    "        \n",
    "        # Domain-specific adaptation\n",
    "        self.domain_embedding = nn.Embedding(2, 512)  # 0: day, 1: night\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec_conv1 = nn.ConvTranspose2d(512, 256, 4, 2, 1)\n",
    "        self.dec_conv2 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
    "        self.dec_conv3 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
    "        self.dec_conv4 = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n",
    "        \n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, domain_label):\n",
    "        # Encoder\n",
    "        e1 = self.leaky_relu(self.enc_conv1(x))\n",
    "        e2 = self.leaky_relu(self.enc_conv2(e1))\n",
    "        e3 = self.leaky_relu(self.enc_conv3(e2))\n",
    "        e4 = self.leaky_relu(self.enc_conv4(e3))\n",
    "        \n",
    "        # Domain conditioning\n",
    "        domain_vec = self.domain_embedding(domain_label).unsqueeze(-1).unsqueeze(-1)\n",
    "        domain_aware_feat = e4 * domain_vec\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attn_map = self.attn(domain_aware_feat)\n",
    "        # print(f\"domain_aware_feat.shape: {domain_aware_feat.shape}\") \n",
    "        # print(f\"attn_map.shape: {attn_map.shape}\")  \n",
    "        attended_feat = domain_aware_feat * attn_map\n",
    "\n",
    "        # Decoder\n",
    "        d1 = self.relu(self.dec_conv1(attended_feat) + e3)\n",
    "        d2 = self.relu(self.dec_conv2(d1) + e2)\n",
    "        d3 = self.relu(self.dec_conv3(d2) + e1)\n",
    "        d4 = self.tanh(self.dec_conv4(d3))\n",
    "        \n",
    "        return d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 4, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(512, 1, 4, 1, 1)\n",
    "        \n",
    "        self.inst_norm = nn.InstanceNorm2d\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.leaky_relu(self.inst_norm(128)(self.conv2(x)))\n",
    "        x = self.leaky_relu(self.inst_norm(256)(self.conv3(x)))\n",
    "        x = self.leaky_relu(self.inst_norm(512)(self.conv4(x)))\n",
    "        x = self.conv5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------- SYSTEMS ---------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaindropRemovalSystem:\n",
    "    def __init__(self):\n",
    "        # self.G_day = DomainAwareGenerator().to(device)\n",
    "        # self.G_night = DomainAwareGenerator().to(device)\n",
    "        self.G = DomainAwareGenerator().to(device) # Single generator for both domains\n",
    "        self.D_clear = MultiScaleDiscriminator().to(device)\n",
    "        self.D_blur = MultiScaleDiscriminator().to(device)\n",
    "        \n",
    "        # self.optim_G = optim.Adam(\n",
    "        #     list(self.G_day.parameters()) + list(self.G_night.parameters()),\n",
    "        #     lr=lr, betas=(0.5, 0.999)\n",
    "        # )\n",
    "        self.optim_G = optim.Adam(self.G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        self.optim_D = optim.Adam(\n",
    "            list(self.D_clear.parameters()) + list(self.D_blur.parameters()),\n",
    "            lr=lr, betas=(0.5, 0.999)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.vgg = models.vgg19(pretrained=True).features[:16].to(device).eval()\n",
    "        self.lpips = lpips.LPIPS(net='vgg').to(device)\n",
    "        \n",
    "        self.criterion_gan = nn.MSELoss()\n",
    "        self.criterion_cycle = nn.L1Loss()\n",
    "        self.criterion_percep = nn.L1Loss()\n",
    "        \n",
    "        self.best_metrics = {'day': {'PSNR': 0, 'SSIM': 0}, 'night': {'PSNR': 0, 'SSIM': 0}}\n",
    "\n",
    "    def compute_metrics(self, pred, target):\n",
    "        pred_np = pred.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "        target_np = target.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "        \n",
    "        psnr_val = np.mean([psnr(t, p, data_range=1.0) for p, t in zip(pred_np, target_np)])\n",
    "        ssim_val = np.mean([ssim(t, p, channel_axis=-1, data_range=1.0) for p, t in zip(pred_np, target_np)])\n",
    "        \n",
    "        lpips_val = self.lpips(pred, target).mean().item()\n",
    "        \n",
    "        return {'PSNR': psnr_val, 'SSIM': ssim_val, 'LPIPS': lpips_val}\n",
    "    \n",
    "    def train_step(self, batch_day, batch_night):\n",
    "        # Prepare data\n",
    "        clear_day = batch_day['clear'].to(device)\n",
    "        blur_day = batch_day['blur'].to(device)\n",
    "        drop_day = batch_day['drop'].to(device)\n",
    "        \n",
    "        clear_night = batch_night['clear'].to(device)\n",
    "        blur_night = batch_night['blur'].to(device)\n",
    "        drop_night = batch_night['drop'].to(device)\n",
    "        \n",
    "        # Domain labels (0: day, 1: night)\n",
    "        day_labels = torch.zeros(drop_day.size(0), dtype=torch.long).to(device)\n",
    "        night_labels = torch.ones(drop_night.size(0), dtype=torch.long).to(device)\n",
    "        \n",
    "        # Train Generators\n",
    "        self.optim_G.zero_grad()\n",
    "        \n",
    "        # Forward passes\n",
    "        restored_day = self.G(drop_day, day_labels)\n",
    "        restored_night = self.G(drop_night, night_labels)\n",
    "        \n",
    "        # Cycle consistency\n",
    "        cycle_day = self.G(restored_day, day_labels)\n",
    "        cycle_night = self.G(restored_night, night_labels)\n",
    "        \n",
    "        # Deblurring\n",
    "        deblur_day = self.G(blur_day, day_labels)\n",
    "        deblur_night = self.G(blur_night, night_labels)\n",
    "        \n",
    "        # Loss calculations\n",
    "        # GAN Loss for clear\n",
    "        g_gan_day = self.criterion_gan(self.D_clear(restored_day), torch.ones_like(self.D_clear(restored_day)))\n",
    "        g_gan_night = self.criterion_gan(self.D_clear(restored_night), torch.ones_like(self.D_clear(restored_night)))\n",
    "        \n",
    "        # GAN Loss for deblur\n",
    "        g_deblur_gan_day = self.criterion_gan(self.D_blur(deblur_day), torch.ones_like(self.D_blur(deblur_day)))\n",
    "        g_deblur_gan_night = self.criterion_gan(self.D_blur(deblur_night), torch.ones_like(self.D_blur(deblur_night)))\n",
    "        \n",
    "        # Cycle Loss\n",
    "        cycle_loss_day = self.criterion_cycle(cycle_day, drop_day)\n",
    "        cycle_loss_night = self.criterion_cycle(cycle_night, drop_night)\n",
    "        \n",
    "        # Perceptual Loss\n",
    "        percep_loss_day = self.criterion_percep(self.vgg(restored_day), self.vgg(clear_day))\n",
    "        percep_loss_night = self.criterion_percep(self.vgg(restored_night), self.vgg(clear_night))\n",
    "        \n",
    "        # Deblur Loss\n",
    "        deblur_loss_day = self.criterion_cycle(deblur_day, clear_day)\n",
    "        deblur_loss_night = self.criterion_cycle(deblur_night, clear_night)\n",
    "        \n",
    "        # Total Generator Loss\n",
    "        g_total = (\n",
    "            g_gan_day + g_gan_night + g_deblur_gan_day + g_deblur_gan_night +\n",
    "            lambda_cycle * (cycle_loss_day + cycle_loss_night) +\n",
    "            lambda_percep * (percep_loss_day + percep_loss_night) +\n",
    "            lambda_blur * (deblur_loss_day + deblur_loss_night)\n",
    "        )\n",
    "        \n",
    "        g_total.backward()\n",
    "        self.optim_G.step()\n",
    "        \n",
    "        # Train Discriminators\n",
    "        self.optim_D.zero_grad()\n",
    "        \n",
    "        # Real data\n",
    "        real_clear_day = self.D_clear(clear_day)\n",
    "        real_clear_night = self.D_clear(clear_night)\n",
    "        d_real_clear = (self.criterion_gan(real_clear_day, torch.ones_like(real_clear_day)) +\n",
    "                       self.criterion_gan(real_clear_night, torch.ones_like(real_clear_night))) / 2\n",
    "        \n",
    "        # Fake data\n",
    "        fake_clear_day = self.D_clear(restored_day.detach())\n",
    "        fake_clear_night = self.D_clear(restored_night.detach())\n",
    "        d_fake_clear = (self.criterion_gan(fake_clear_day, torch.zeros_like(fake_clear_day)) +\n",
    "                       self.criterion_gan(fake_clear_night, torch.zeros_like(fake_clear_night))) / 2\n",
    "        \n",
    "        # Loss for D_clear\n",
    "        d_clear_loss = 0.5 * (d_real_clear + d_fake_clear)\n",
    "        \n",
    "        # Loss for D_blur (deblur)\n",
    "        real_blur_day = self.D_blur(blur_day)\n",
    "        real_blur_night = self.D_blur(blur_night)\n",
    "        d_real_blur = (self.criterion_gan(real_blur_day, torch.ones_like(real_blur_day)) +\n",
    "                      self.criterion_gan(real_blur_night, torch.ones_like(real_blur_night))) / 2\n",
    "        \n",
    "        fake_deblur_day = self.D_blur(deblur_day.detach())\n",
    "        fake_deblur_night = self.D_blur(deblur_night.detach())\n",
    "        d_fake_blur = (self.criterion_gan(fake_deblur_day, torch.zeros_like(fake_deblur_day)) +\n",
    "                      self.criterion_gan(fake_deblur_night, torch.zeros_like(fake_deblur_night))) / 2\n",
    "        \n",
    "        d_blur_loss = 0.5 * (d_real_blur + d_fake_blur)\n",
    "        \n",
    "        # Tá»•ng Discriminator Loss\n",
    "        d_total = d_clear_loss + d_blur_loss\n",
    "        d_total.backward()\n",
    "        self.optim_D.step()\n",
    "        \n",
    "        return {\n",
    "            'g_total': g_total.item(),\n",
    "            'd_total': d_total.item(),\n",
    "            'cycle_day': cycle_loss_day.item(),\n",
    "            'percep_day': percep_loss_day.item()\n",
    "        }\n",
    "\n",
    "    def validate(self, val_loader, domain):\n",
    "        self.G.eval()\n",
    "        \n",
    "        total_metrics = {'PSNR': 0, 'SSIM': 0, 'LPIPS': 0}\n",
    "        domain_label = torch.zeros(1, dtype=torch.long).to(device) if domain == 'day' else torch.ones(1, dtype=torch.long).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch['drop'].to(device)\n",
    "                targets = batch['clear'].to(device)\n",
    "                \n",
    "                outputs = self.G(inputs, domain_label.expand(inputs.size(0)))\n",
    "                \n",
    "                metrics = self.compute_metrics(outputs, targets)\n",
    "                for k in total_metrics:\n",
    "                    total_metrics[k] += metrics[k]\n",
    "                    \n",
    "        for k in total_metrics:\n",
    "            total_metrics[k] /= len(val_loader)\n",
    "            \n",
    "        return total_metrics\n",
    "\n",
    "    def save_checkpoint(self, path, epoch, is_best=False):\n",
    "        state = {\n",
    "            'G': self.G.state_dict(),\n",
    "            'D_clear': self.D_clear.state_dict(),\n",
    "            'D_blur': self.D_blur.state_dict(),\n",
    "            'optim_G': self.optim_G.state_dict(),\n",
    "            'optim_D': self.optim_D.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'best_metrics': self.best_metrics\n",
    "        }\n",
    "        torch.save(state, path)\n",
    "        if is_best:\n",
    "            torch.save(state, \"best_model.pth\")\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.G.load_state_dict(checkpoint['G'])\n",
    "        self.D_clear.load_state_dict(checkpoint['D_clear'])\n",
    "        self.D_blur.load_state_dict(checkpoint['D_blur'])\n",
    "        self.optim_G.load_state_dict(checkpoint['optim_G'])\n",
    "        self.optim_D.load_state_dict(checkpoint['optim_D'])\n",
    "        self.best_metrics = checkpoint['best_metrics']\n",
    "        return checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------- TRAINING SETUP ---------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Prepare datasets\n",
    "    def get_loaders(path):\n",
    "        full_ds = MultiDomainRaindropDataset(path)\n",
    "        train_size = int((1 - val_ratio) * len(full_ds))\n",
    "        val_size = len(full_ds) - train_size\n",
    "        train_ds, val_ds = random_split(full_ds, [train_size, val_size])\n",
    "        return DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True), \\\n",
    "               DataLoader(val_ds, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    day_train_loader, day_val_loader = get_loaders('/kaggle/input/raindrop-daynight/DayRainDrop_Train/DayRainDrop_Train')\n",
    "    night_train_loader, night_val_loader = get_loaders('/kaggle/input/raindrop-daynight/NightRainDrop_Train/NightRainDrop_Train')\n",
    "\n",
    "    # Initialize system\n",
    "    system = RaindropRemovalSystem()\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Main training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # Training\n",
    "        system.G.train()\n",
    "        \n",
    "        progress = tqdm(zip(day_train_loader, night_train_loader), \n",
    "                        total=min(len(day_train_loader), len(night_train_loader)), desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        for batch_idx, (batch_day, batch_night) in enumerate(progress):\n",
    "            losses = system.train_step(batch_day, batch_night)\n",
    "            \n",
    "            progress.set_description(\n",
    "                f\"Epoch {epoch} | G: {losses['g_total']:.3f} | D: {losses['d_total']:.3f} | \"\n",
    "                f\"Cycle: {losses['cycle_day']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        # Validation\n",
    "        day_metrics = system.validate(day_val_loader, 'day')\n",
    "        night_metrics = system.validate(night_val_loader, 'night')\n",
    "        \n",
    "        # Save best models\n",
    "        if day_metrics['PSNR'] > system.best_metrics['day']['PSNR']:\n",
    "            system.best_metrics['day'] = day_metrics\n",
    "            system.save_checkpoint(f\"best_day_model_epoch{epoch}.pth\", epoch, is_best=True)\n",
    "            \n",
    "        if night_metrics['PSNR'] > system.best_metrics['night']['PSNR']:\n",
    "            system.best_metrics['night'] = night_metrics\n",
    "            system.save_checkpoint(f\"best_night_model_epoch{epoch}.pth\", epoch, is_best=True)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nValidation @ Epoch {epoch}:\")\n",
    "        print(f\"[Day] PSNR: {day_metrics['PSNR']:.2f} | SSIM: {day_metrics['SSIM']:.4f} | LPIPS: {day_metrics['LPIPS']:.4f}\")\n",
    "        print(f\"[Night] PSNR: {night_metrics['PSNR']:.2f} | SSIM: {night_metrics['SSIM']:.4f} | LPIPS: {night_metrics['LPIPS']:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if epoch % 10 == 0:\n",
    "            system.save_checkpoint(f\"checkpoint_epoch{epoch}.pth\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
